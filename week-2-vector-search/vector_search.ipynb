{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1e3b268",
      "metadata": {},
      "source": [
        "### What is vector seach?\n",
        "\n",
        "Vector search leverages machine learning (ML) to capture the meaning and context of unstructured data, including text and images, transforming it into a numeric representation. Frequently used for semantic search, vector search finds similar data using approximate nearest neighbor (ANN) algorithms. Compared to traditional keyword search, vector search yields more relevant results and executes faster.\n",
        "\n",
        "`semantic search`\n",
        " is a search method that focuses on understanding the meaning and intent behind a user's search query, rather than just matching keywords.\n",
        "\n",
        "`Vector search` is a technique where text (or images, code, etc.) is converted into numerical vectors, and search is done by comparing the similarity of vectors — instead of matching words.\n",
        "\n",
        "```It lets you search by meaning, not just keywords.```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac13c24",
      "metadata": {},
      "source": [
        "### What is an Embedding?\n",
        "\n",
        "To apply `vector search` we need to embed the unstructured data we want to perform our vector search on.\n",
        "\n",
        "An `embedding` is just a list of numbers that represents the meaning of your text.\n",
        "\n",
        "for example\n",
        "\n",
        "``` \"The cat is cute\" → [0.12, -0.44, 0.91, ..., 0.03] ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e3fc0ae",
      "metadata": {},
      "source": [
        "### What is an Embedding Model?\n",
        "An embedding model is a special AI model that creates these embeddings.\n",
        "\n",
        "It takes text like this:\n",
        "\n",
        "\n",
        "``` \"The cat is cute\" ```\n",
        "\n",
        "And turns it into something like this:\n",
        "\n",
        "``` [0.12, -0.44, 0.91, ..., 0.03] ```  \n",
        "\n",
        "You can think of the embedding model as:\n",
        "\n",
        "A translator that turns words into vectors (math) that computers can search, compare, and rank.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba03652",
      "metadata": {},
      "source": [
        "After vectorizing our data, we can perform operations like similarity search. Tools like Qdrant help by storing and indexing these vectors for fast and efficient retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb21fe3",
      "metadata": {},
      "source": [
        "### What is Qdrant?\n",
        "\n",
        "Qdrant “is a vector similarity search engine that provides a production-ready service with a convenient API to store, search, and manage points (i.e. vectors) with an additional payload.” You can think of the payloads as additional pieces of information that can help you hone in on your search and also receive useful information that you can give to your users.\n",
        "\n",
        "Qdrant’s Role:\n",
        "\n",
        "- Qdrant is a vector database.\n",
        "\n",
        "    It can:\n",
        "\n",
        "    ✅ Store vectors (and associated metadata, aka “payload”)\n",
        "\n",
        "    ✅ Index them efficiently\n",
        "\n",
        "    ✅ Search them by similarity (e.g. cosine similarity)\n",
        "\n",
        "    ✅ Filter results using metadata (e.g. filter by language, category, etc.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4701dcc8",
      "metadata": {},
      "source": [
        "### Qdrant Vs Elasticsearch\n",
        "\n",
        "`Qdrant` and `Elasticsearch` differ in how they index and search data:\n",
        "\n",
        "`Elasticsearch` is optimized for keyword-based search, using inverted indexes to match exact terms or phrases.\n",
        "\n",
        "`Qdrant`, on the other hand, is designed for semantic search, using vector indexes to find similar meanings rather than exact words.\n",
        "\n",
        "#### `Even Simpler Version`:\n",
        "\n",
        "`Elasticsearch` finds documents by matching words.\n",
        "`Qdrant` finds documents by matching meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d4a56f",
      "metadata": {},
      "source": [
        "### let's look at the 2 steps to do a vector search \n",
        "\n",
        "(1) Indexing Stored Data (One-Time Setup) (your knowledg db)\n",
        "\n",
        "- This is where you prepare your documents for search.\n",
        "\n",
        "`Document Chunk → Embedding Model → Vector → Qdrant (Store)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed7398a",
      "metadata": {},
      "source": [
        "\n",
        "(2) Searching with a Query (Repeated at runtime)\n",
        "- This happens when a user asks a question or wants to search.\n",
        "\n",
        "`User Query → Embedding Model → Vector → Qdrant (Search) → Matching Text`\n",
        "\n",
        "\n",
        "`Embedding Model` A model that converts text into vectors based on meaning\n",
        "\n",
        "`Vector` A list of numbers (like [0.24, -0.18, 0.91, ...]) that encodes meaning\n",
        "\n",
        "`Qdrant` A vector database: stores, indexes, and searches over vectors\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78170a3c",
      "metadata": {},
      "source": [
        "### Let's start working with qdrant.\n",
        "\n",
        "`quickstart`\n",
        "\n",
        "- Use the following link to quickly set up you Qdrant.\n",
        "\n",
        "https://qdrant.tech/documentation/quickstart/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8926273a",
      "metadata": {},
      "source": [
        "### Set up python client \n",
        "\n",
        "- install required library\n",
        "\n",
        "        ```pip install -q \"qdrant-client[fastembed]>=1.14.2\"```\n",
        "\n",
        "- It provides a client interface so your Python code can easily talk to the Qdrant server via its API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "b4c40a3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "python(42904) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \"qdrant-client[fastembed]>=1.14.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b74d0e5",
      "metadata": {},
      "source": [
        "### Step 1. Import Required Libraries & Connect to Qdrant\n",
        "\n",
        "Now let’s import the necessary modules from the qdrant-client package.\n",
        "\n",
        "The QdrantClient class allows us to establish a connection to the Qdrant service,\n",
        "while the models module provides definitions for various configurations and parameters we’ll use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "529137b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "client = QdrantClient(\"http://localhost:6333\") #connecting to local Qdrant instance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b561d8a",
      "metadata": {},
      "source": [
        "### Step 2: Study the Dataset\n",
        "To build a working vector search solution (and, more generally, to understand if/when/how it’s needed), it's good to study the dataset and figure out the nature and structure of the data we’re working with, for example:\n",
        "\n",
        "- modality — is it text, images, videos, a combination?\n",
        "- specifics — if it’s text: language used, how big are the text pieces, are there any special characters, etc.\n",
        "It will help us define:\n",
        "\n",
        "- the right data \"schema\" (what to vectorize, what to store as metadata, etc);\n",
        "- the right embedding model (the best fit based on the domain, precision & resource requirements).\n",
        "We have a toy dataset provided for experimentation, let's check it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "d0d4c446",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
        "docs_response = requests.get(docs_url)\n",
        "documents_raw = docs_response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "be615d46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# documents_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1343098d",
      "metadata": {},
      "source": [
        "### Step 3. Choosing embeddding model\n",
        "\n",
        "Embedding Model = A tool that turns text into numbers (meaningful vector).\n",
        " exmaple: \n",
        "- `text-embedding-ada-002` by OpenAI \n",
        "\n",
        "- Qdrant's `FastEmbed`\n",
        "\n",
        "- `transformers` by Hugging Face \n",
        "\n",
        "\n",
        "The choice of an embedding model depends on many factors:\n",
        "\n",
        "The task, data modality, and data specifics;\n",
        "The trade-off between search precision and resource usage (larger embeddings require more storage and memory);\n",
        "The cost of inference (especially if you're using a third-party provider);\n",
        "\n",
        "> The best way to select an embedding model is to test and benchmark different options on your own data.\n",
        "\n",
        "\n",
        "\n",
        "In this particular study we will use `Qdrant's FastEmbed`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a54f692",
      "metadata": {},
      "source": [
        "`FastEmbed` is an optimized embedding solution designed specifically for Qdrant. It delivers low-latency, CPU-friendly embedding generation, eliminating the need for heavy frameworks like PyTorch or TensorFlow. It uses quantized model weights and ONNX Runtime, making it significantly faster than traditional Sentence Transformers on CPU while maintaining competitive accuracy.\n",
        "\n",
        "FastEmbed supports:\n",
        "\n",
        "`Dense embeddings` for text and images (the most common type in vector search, ones we're going to use today)\n",
        "\n",
        "`Sparse embeddings` (e.g., BM25 and sparse neural embeddings)\n",
        "\n",
        "`Multivector embeddings` (e.g., ColPali and ColBERT, late interaction models)\n",
        "\n",
        "`Rerankers`\n",
        "\n",
        "All of these can be directly used in Qdrant (as Qdrant supports dense, sparse & multivectors along with hybrid search).\n",
        "FastEmbed’s integration with Qdrant allows you to directly pass text or images to the Qdrant client for embedding.\n",
        "\n",
        "In this notebook, we’ll use FastEmbed for local inference with Qdrant.\n",
        "\n",
        "> Keep in mind your machine's resources when choosing an embedding model for local inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f010802",
      "metadata": {},
      "source": [
        "##### FastEmbed for Textual Data\n",
        "Let’s select an embedding model to use for our course question answers, stored in text fields, from the options supported by FastEmbed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "10bca4d1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'model': 'BAAI/bge-base-en',\n",
              "  'sources': {'hf': 'Qdrant/fast-bge-base-en',\n",
              "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz',\n",
              "   '_deprecated_tar_struct': True},\n",
              "  'model_file': 'model_optimized.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 0.42,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'BAAI/bge-base-en-v1.5',\n",
              "  'sources': {'hf': 'qdrant/bge-base-en-v1.5-onnx-q',\n",
              "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz',\n",
              "   '_deprecated_tar_struct': True},\n",
              "  'model_file': 'model_optimized.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 0.21,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'BAAI/bge-large-en-v1.5',\n",
              "  'sources': {'hf': 'qdrant/bge-large-en-v1.5-onnx',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 1.2,\n",
              "  'additional_files': [],\n",
              "  'dim': 1024,\n",
              "  'tasks': {}},\n",
              " {'model': 'BAAI/bge-small-en',\n",
              "  'sources': {'hf': 'Qdrant/bge-small-en',\n",
              "   'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz',\n",
              "   '_deprecated_tar_struct': True},\n",
              "  'model_file': 'model_optimized.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2023 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 0.13,\n",
              "  'additional_files': [],\n",
              "  'dim': 384,\n",
              "  'tasks': {}},\n",
              " {'model': 'BAAI/bge-small-en-v1.5',\n",
              "  'sources': {'hf': 'qdrant/bge-small-en-v1.5-onnx-q',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'model_optimized.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 0.067,\n",
              "  'additional_files': [],\n",
              "  'dim': 384,\n",
              "  'tasks': {}},\n",
              " {'model': 'BAAI/bge-small-zh-v1.5',\n",
              "  'sources': {'hf': 'Qdrant/bge-small-zh-v1.5',\n",
              "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz',\n",
              "   '_deprecated_tar_struct': True},\n",
              "  'model_file': 'model_optimized.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 0.09,\n",
              "  'additional_files': [],\n",
              "  'dim': 512,\n",
              "  'tasks': {}},\n",
              " {'model': 'mixedbread-ai/mxbai-embed-large-v1',\n",
              "  'sources': {'hf': 'mixedbread-ai/mxbai-embed-large-v1',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.64,\n",
              "  'additional_files': [],\n",
              "  'dim': 1024,\n",
              "  'tasks': {}},\n",
              " {'model': 'snowflake/snowflake-arctic-embed-xs',\n",
              "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-xs',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.09,\n",
              "  'additional_files': [],\n",
              "  'dim': 384,\n",
              "  'tasks': {}},\n",
              " {'model': 'snowflake/snowflake-arctic-embed-s',\n",
              "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-s',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.13,\n",
              "  'additional_files': [],\n",
              "  'dim': 384,\n",
              "  'tasks': {}},\n",
              " {'model': 'snowflake/snowflake-arctic-embed-m',\n",
              "  'sources': {'hf': 'Snowflake/snowflake-arctic-embed-m',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.43,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'snowflake/snowflake-arctic-embed-m-long',\n",
              "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-m-long',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 2048 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.54,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'snowflake/snowflake-arctic-embed-l',\n",
              "  'sources': {'hf': 'snowflake/snowflake-arctic-embed-l',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 1.02,\n",
              "  'additional_files': [],\n",
              "  'dim': 1024,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-clip-v1',\n",
              "  'sources': {'hf': 'jinaai/jina-clip-v1',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/text_model.onnx',\n",
              "  'description': 'Text embeddings, Multimodal (text&image), English, Prefixes for queries/documents: not necessary, 2024 year',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.55,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'Qdrant/clip-ViT-B-32-text',\n",
              "  'sources': {'hf': 'Qdrant/clip-ViT-B-32-text',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'model.onnx',\n",
              "  'description': 'Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 0.25,\n",
              "  'additional_files': [],\n",
              "  'dim': 512,\n",
              "  'tasks': {}},\n",
              " {'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
              "  'sources': {'hf': 'qdrant/all-MiniLM-L6-v2-onnx',\n",
              "   'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz',\n",
              "   '_deprecated_tar_struct': True},\n",
              "  'model_file': 'model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 256 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.09,\n",
              "  'additional_files': [],\n",
              "  'dim': 384,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-embeddings-v2-base-en',\n",
              "  'sources': {'hf': 'xenova/jina-embeddings-v2-base-en',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.52,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-embeddings-v2-small-en',\n",
              "  'sources': {'hf': 'xenova/jina-embeddings-v2-small-en',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.12,\n",
              "  'additional_files': [],\n",
              "  'dim': 512,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-embeddings-v2-base-de',\n",
              "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-de',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model_fp16.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), Multilingual (German, English), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.32,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-embeddings-v2-base-code',\n",
              "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-code',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), Multilingual (English, 30 programming languages), 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.64,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-embeddings-v2-base-zh',\n",
              "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-zh',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), supports mixed Chinese-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.64,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-embeddings-v2-base-es',\n",
              "  'sources': {'hf': 'jinaai/jina-embeddings-v2-base-es',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), supports mixed Spanish-English input text, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.64,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'thenlper/gte-base',\n",
              "  'sources': {'hf': 'thenlper/gte-base',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'General text embeddings, Unimodal (text), supports English only input text, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2024 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 0.44,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'thenlper/gte-large',\n",
              "  'sources': {'hf': 'qdrant/gte-large-onnx',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 1.2,\n",
              "  'additional_files': [],\n",
              "  'dim': 1024,\n",
              "  'tasks': {}},\n",
              " {'model': 'nomic-ai/nomic-embed-text-v1.5',\n",
              "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.52,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'nomic-ai/nomic-embed-text-v1.5-Q',\n",
              "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model_quantized.onnx',\n",
              "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.13,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'nomic-ai/nomic-embed-text-v1',\n",
              "  'sources': {'hf': 'nomic-ai/nomic-embed-text-v1',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.52,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
              "  'sources': {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'model_optimized.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 512 input tokens truncation, Prefixes for queries/documents: not necessary, 2019 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 0.22,\n",
              "  'additional_files': [],\n",
              "  'dim': 384,\n",
              "  'tasks': {}},\n",
              " {'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
              "  'sources': {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 languages), 384 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year.',\n",
              "  'license': 'apache-2.0',\n",
              "  'size_in_GB': 1.0,\n",
              "  'additional_files': [],\n",
              "  'dim': 768,\n",
              "  'tasks': {}},\n",
              " {'model': 'intfloat/multilingual-e5-large',\n",
              "  'sources': {'hf': 'qdrant/multilingual-e5-large-onnx',\n",
              "   'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz',\n",
              "   '_deprecated_tar_struct': True},\n",
              "  'model_file': 'model.onnx',\n",
              "  'description': 'Text embeddings, Unimodal (text), Multilingual (~100 languages), 512 input tokens truncation, Prefixes for queries/documents: necessary, 2024 year.',\n",
              "  'license': 'mit',\n",
              "  'size_in_GB': 2.24,\n",
              "  'additional_files': ['model.onnx_data'],\n",
              "  'dim': 1024,\n",
              "  'tasks': {}},\n",
              " {'model': 'jinaai/jina-embeddings-v3',\n",
              "  'sources': {'hf': 'jinaai/jina-embeddings-v3',\n",
              "   'url': None,\n",
              "   '_deprecated_tar_struct': False},\n",
              "  'model_file': 'onnx/model.onnx',\n",
              "  'description': 'Multi-task unimodal (text) embedding model, multi-lingual (~100), 1024 tokens truncation, and 8192 sequence length. Prefixes for queries/documents: not necessary, 2024 year.',\n",
              "  'license': 'cc-by-nc-4.0',\n",
              "  'size_in_GB': 2.29,\n",
              "  'additional_files': ['onnx/model.onnx_data'],\n",
              "  'dim': 1024,\n",
              "  'tasks': {'retrieval.query': 0,\n",
              "   'retrieval.passage': 1,\n",
              "   'separation': 2,\n",
              "   'classification': 3,\n",
              "   'text-matching': 4}}]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from fastembed import TextEmbedding\n",
        "TextEmbedding.list_supported_models() # shows you different types of textembedding models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6ebdf6c",
      "metadata": {},
      "source": [
        "It makes sense to choose a model that produces small-to-moderate-sized embeddings (e.g., 512 dimensions), so we don’t overuse resources in our simple setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2d43e0d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"model\": \"BAAI/bge-small-zh-v1.5\",\n",
            "  \"sources\": {\n",
            "    \"hf\": \"Qdrant/bge-small-zh-v1.5\",\n",
            "    \"url\": \"https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz\",\n",
            "    \"_deprecated_tar_struct\": true\n",
            "  },\n",
            "  \"model_file\": \"model_optimized.onnx\",\n",
            "  \"description\": \"Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.\",\n",
            "  \"license\": \"mit\",\n",
            "  \"size_in_GB\": 0.09,\n",
            "  \"additional_files\": [],\n",
            "  \"dim\": 512,\n",
            "  \"tasks\": {}\n",
            "}\n",
            "{\n",
            "  \"model\": \"Qdrant/clip-ViT-B-32-text\",\n",
            "  \"sources\": {\n",
            "    \"hf\": \"Qdrant/clip-ViT-B-32-text\",\n",
            "    \"url\": null,\n",
            "    \"_deprecated_tar_struct\": false\n",
            "  },\n",
            "  \"model_file\": \"model.onnx\",\n",
            "  \"description\": \"Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year\",\n",
            "  \"license\": \"mit\",\n",
            "  \"size_in_GB\": 0.25,\n",
            "  \"additional_files\": [],\n",
            "  \"dim\": 512,\n",
            "  \"tasks\": {}\n",
            "}\n",
            "{\n",
            "  \"model\": \"jinaai/jina-embeddings-v2-small-en\",\n",
            "  \"sources\": {\n",
            "    \"hf\": \"xenova/jina-embeddings-v2-small-en\",\n",
            "    \"url\": null,\n",
            "    \"_deprecated_tar_struct\": false\n",
            "  },\n",
            "  \"model_file\": \"onnx/model.onnx\",\n",
            "  \"description\": \"Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.\",\n",
            "  \"license\": \"apache-2.0\",\n",
            "  \"size_in_GB\": 0.12,\n",
            "  \"additional_files\": [],\n",
            "  \"dim\": 512,\n",
            "  \"tasks\": {}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "EMBEDDING_DIMENSIONALITY = 512\n",
        "\n",
        "for model in TextEmbedding.list_supported_models():\n",
        "    if model[\"dim\"] == EMBEDDING_DIMENSIONALITY:\n",
        "        print(json.dumps(model, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c12648a",
      "metadata": {},
      "source": [
        "> We need an embedding model suitable for English text.\n",
        "\n",
        "It also makes sense to select a unimodal model, since we’re not including images in our search, and specifically tailored solutions are usually better than universal ones.\n",
        "\n",
        "> It seems like `jina-embedding-small-en` is a good choice!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1ef9e9",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "58e28d49",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2d0fcd",
      "metadata": {},
      "source": [
        "Like most dense embedding models, `jina-embedding-small-en` was trained to measure semantic closeness using **cosine similarity**.\n",
        "\n",
        "> The parameters of the chosen embedding model, including the output embedding dimensions and the semantic similarity (distance) metric, are required to configure semantic search in Qdrant.\n",
        "\n",
        "Now we’re ready to configure and use Qdrant for semantic search. To fully understand what’s happening, here’s a quick overview of Qdrant’s core terminology:\n",
        "\n",
        "`Points` are the central entity Qdrant works with.\n",
        "\n",
        "A point is a record consisting of an `ID`, a `vector`, and an optional `payload`.\n",
        "\n",
        "A `collection` is a named set of points (i.e., vectors with optional payloads) that you can search within.\n",
        "\n",
        "Think of it as the `container` for your vector search solution, `a single business problem solved`.\n",
        "\n",
        "> Qdrant supports different types of vectors to enable different modes of data exploration and search (dense, sparse, multivectors, and named vectors).\n",
        "\n",
        "In this example, we’ll use the most common type, `dense vectors`.\n",
        "\n",
        "\n",
        "Embeddings capture the semantic essence of the data, while the `payload` holds structured metadata.\n",
        "\n",
        "This metadata becomes especially useful when applying filters or sorting during search. `Qdrant's payloads` can hold structured data like `booleans`, `keywords`, `geo-locations`, `arrays`, and `nested objects`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a5cfa1",
      "metadata": {},
      "source": [
        "### Step 4: Create a Collection\n",
        "When creating a [collection](https://qdrant.tech/documentation/concepts/collections/), we need to specify:\n",
        "\n",
        "`Name`: A unique identifier for the collection.\n",
        "\n",
        "`Vector Configuration`:\n",
        "\n",
        "- `Size`: The dimensionality of the vectors.\n",
        "- `Distance Metric`: The method used to measure similarity between vectors.\n",
        "    \n",
        "There are additional parameters you can explore in our documentation. Moreover, you can configure other vector types in Qdrant beyond typical dense embeddings (f.e., for hybrid search). However, for this example, the simplest default configuration is sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "88ffe6f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection already exists. Skipping creation.\n"
          ]
        }
      ],
      "source": [
        "# Define the collection name\n",
        "collection_name = \"zoomcamp-rag\"\n",
        "\n",
        "# Create the collection with specified vector parameters\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "\n",
        "\n",
        "try: \n",
        "    client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=models.VectorParams(\n",
        "            size = EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
        "            distance=models.Distance.COSINE  # Distance metric for similarity search\n",
        "        )\n",
        "    )\n",
        "except UnexpectedResponse as e:\n",
        "    if \"already exists\" in str(e):\n",
        "        print(\"Collection already exists. Skipping creation.\")\n",
        "    else:\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "864aeb2f",
      "metadata": {},
      "source": [
        "### Step 5: Create, Embed & Insert Points into the Collection\n",
        "\n",
        "`Points` are the core data entities in Qdrant. Each point consists of:\n",
        "\n",
        "- `ID`. A unique identifier. Qdrant supports both 64-bit unsigned integers and UUIDs.\n",
        "\n",
        "- `Vector`. The embedding that represents the data point in vector space.\n",
        "\n",
        "- `Payload` (optional). Additional metadata as key-value pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "d2ebad30",
      "metadata": {},
      "outputs": [],
      "source": [
        "points = []\n",
        "id = 0\n",
        "\n",
        "for course in documents_raw:\n",
        "    for doc in course['documents']:\n",
        "\n",
        "        point = models.PointStruct(\n",
        "            id=id,\n",
        "            vector=models.Document(text=doc['text'], model=model_handle), #embed text locally with \"jinaai/jina-embeddings-v2-small-en\" from FastEmbed\n",
        "            payload={\n",
        "                \"text\": doc['text'],\n",
        "                \"section\": doc['section'],\n",
        "                \"course\": course['course']\n",
        "            } #save all needed metadata fields\n",
        "        )\n",
        "        points.append(point)\n",
        "\n",
        "        id += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d3e2f02",
      "metadata": {},
      "source": [
        "Now we’re going to embed and upload points to our collection.\n",
        "\n",
        "\n",
        "First, FastEmbed will fetch&download the selected model (path defaults to `os.path.join(tempfile.gettempdir(), \"fastembed_cache\")`), and perform inference directly on your machine.\n",
        "\n",
        "Then, the generated points will be upserted into the collection, and the vector index will be built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "161fc8bc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=4, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.upsert(\n",
        "    collection_name=collection_name,\n",
        "    points=points\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4481bc7",
      "metadata": {},
      "source": [
        "\n",
        "The speed of upsert mainly depends on the time spent on local inference.\n",
        "\n",
        "To speed this up, you could run FastEmbed on GPUs or use a machine with more resources.\n",
        "\n",
        "In addition to basic upsert, Qdrant supports `batch upsert` in both column- and record-oriented formats.\n",
        "\n",
        "The Python client offers:\n",
        "\n",
        "- Parallelization\n",
        "\n",
        "- Retries\n",
        "\n",
        "- Lazy batching\n",
        "\n",
        "These can be configured via parameters in the upload_collection and upload_points functions.\n",
        "For details, check the documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a2de6f",
      "metadata": {},
      "source": [
        "### Study Data Visually\n",
        "Let’s explore the uploaded data in the Qdrant Web UI at http://localhost:6333/dashboard to study semantic similarity visually.\n",
        "\n",
        "For example, using the `Visualize` tab in the `zoomcamp-rag` collection, we can view all answers to the course questions (948 points) and see how they group together by meaning, additionally coloured by the course type.\n",
        "\n",
        "To do that, run the following command:\n",
        "``` json\n",
        "{\n",
        "  \"limit\": 948,\n",
        "  \"color_by\": {\n",
        "    \"payload\": \"course\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "This 2D representation is the result of dimensionality reduction applied to `jina-embeddings`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb81f019",
      "metadata": {},
      "source": [
        "### Step 6: Running a Similarity Search\n",
        "Now, let’s find the most similar text vector in Qdrant to a given query embedding - the most relevant answer to a given question.\n",
        "\n",
        "##### How Similarity Search Works\n",
        "\n",
        "1. Qdrant compares the query vector to stored vectors (based on a vector index) using the distance metric defined when creating the collection.\n",
        "\n",
        "2. The closest matches are returned, ranked by similarity.\n",
        "\n",
        "> Vector index is built for `approximate nearest neighbor (ANN)` search, making large-scale vector search feasible.\n",
        "\n",
        "If you'd like to dive into our choice of vector index for vector search, check our article[\"What is a vector database\"](https://qdrant.tech/articles/what-is-a-vector-database/), or, for a more technical deep dive, our article on [Filterable Hierarchical Navigable Small World](https://qdrant.tech/articles/filtrable-hnsw/).\n",
        "\n",
        "Let's define a search function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "8c21cedd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search(query, limit=1):\n",
        "\n",
        "    results = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=models.Document( #embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
        "            text=query,\n",
        "            model=model_handle \n",
        "        ),\n",
        "        limit=limit, # top closest matches\n",
        "        with_payload=True #to get metadata in the results\n",
        "    )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "318cee41",
      "metadata": {},
      "source": [
        "Now let’s pick a random question from the course data.\n",
        "\n",
        "As you remember, we didn’t upload the questions to Qdrant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "415bb719",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"text\": \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\",\n",
            "  \"section\": \"Module 1: Docker and Terraform\",\n",
            "  \"question\": \"Docker - Cannot pip install on Docker container (Windows)\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "course = random.choice(documents_raw)\n",
        "course_piece = random.choice(course['documents'])\n",
        "print(json.dumps(course_piece, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20335eff",
      "metadata": {},
      "source": [
        "Let's see which answer we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "d1f11647",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = search(course_piece['question'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "2f64af6a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QueryResponse(points=[ScoredPoint(id=802, version=4, score=0.8624964, payload={'text': 'When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\\n```\\nWarning: Python 3.11 was not found on your system…\\nNeither ‘pipenv’ nor ‘asdf’ could be found to install Python.\\nYou can specify specific versions of Python with:\\n$ pipenv –python path\\\\to\\\\python\\n```\\nThe solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\\n(Added by Abhijit Chakraborty)', 'section': 'Miscellaneous', 'course': 'machine-learning-zoomcamp'}, vector=None, shard_key=None, order_value=None)])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f583a2c9",
      "metadata": {},
      "source": [
        "**score** – the `cosine similarity` between the `question` and `text` embeddings.\n",
        "\n",
        "Let’s compare the original and retrieved answers for our randomly selected question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "06efcff4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "Docker - Cannot pip install on Docker container (Windows)\n",
            "\n",
            "Top Retrieved Answer:\n",
            "When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\n",
            "```\n",
            "Warning: Python 3.11 was not found on your system…\n",
            "Neither ‘pipenv’ nor ‘asdf’ could be found to install Python.\n",
            "You can specify specific versions of Python with:\n",
            "$ pipenv –python path\\to\\python\n",
            "```\n",
            "The solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\n",
            "(Added by Abhijit Chakraborty)\n",
            "\n",
            "Original Answer:\n",
            "You may have this error:\n",
            "Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\n",
            "rllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n",
            "/simple/pandas/\n",
            "Possible solution might be:\n",
            "$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\n"
          ]
        }
      ],
      "source": [
        "print(f\"Question:\\n{course_piece['question']}\\n\")\n",
        "print(\"Top Retrieved Answer:\\n{}\\n\".format(result.points[0].payload['text']))\n",
        "print(\"Original Answer:\\n{}\".format(course_piece['text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6af45e4",
      "metadata": {},
      "source": [
        "Now let’s search the answer to a question that wasn’t in the initial dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "e71bf403",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\n",
            "Older news:[source1] [source2]\n"
          ]
        }
      ],
      "source": [
        "print(search(\"What if I submit homeworks late?\").points[0].payload['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f3cfcc",
      "metadata": {},
      "source": [
        "### Step 7: Running a Similarity Search with Filters\n",
        "\n",
        "We can refine our search using metadata filters.\n",
        "\n",
        "> Qdrant’s custom vector index implementation, Filterable HNSW, allows for precise and scalable vector search with filtering conditions.\n",
        "\n",
        "For example, we can search for an answer to a question related to a specific course from the three available in the dataset.\n",
        "Using a `mus`t filter ensures that all specified conditions are met for a data point to be included in the search results.\n",
        "\n",
        "> Qdrant also supports other filter types such as `should`, `must_not`, `range`, and more. For a full overview, check our [Filtering Guide](https://qdrant.tech/articles/vector-search-filtering/)\n",
        "\n",
        "To enable efficient filtering, we need to turn on [indexing of payload fields](https://qdrant.tech/documentation/concepts/indexing/#payload-index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "114eb7d8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=6, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.create_payload_index(\n",
        "    collection_name=collection_name,\n",
        "    field_name=\"course\",\n",
        "    field_schema=\"keyword\" # exact matching on string metadata fields\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e096d51b",
      "metadata": {},
      "source": [
        "Now let's update our search function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "3851e1e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_in_course(query, course=\"mlops-zoomcamp\", limit=1):\n",
        "\n",
        "    results = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=models.Document( #embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
        "            text=query,\n",
        "            model=model_handle\n",
        "        ),\n",
        "        query_filter=models.Filter( # filter by course name\n",
        "            must=[\n",
        "                models.FieldCondition(\n",
        "                    key=\"course\",\n",
        "                    match=models.MatchValue(value=course)\n",
        "                )\n",
        "            ]\n",
        "        ),\n",
        "        limit=limit, # top closest matches\n",
        "        with_payload=True #to get metadata in the results\n",
        "    )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9a5df0",
      "metadata": {},
      "source": [
        "Let’s see how the same question is answered across different courses:\n",
        "\n",
        "`data-engineering-zoomcamp`,` machine-learning-zoomcamp`, and `mlops-zoomcamp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "d22dd442",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please choose the closest one to your answer. Also do not post your answer in the course slack channel.\n"
          ]
        }
      ],
      "source": [
        "print(search_in_course(\"What if I submit homeworks late?\", \"mlops-zoomcamp\").points[0].payload['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab1d0a5",
      "metadata": {},
      "source": [
        "# done!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df0f0133",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
