{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df4505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"dlt[qdrant]\" \"qdrant-client[fastembed]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dcf65",
   "metadata": {},
   "source": [
    "### Question 1. dlt Version\n",
    "##### What's the version of dlt that you installed?\n",
    "###### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4aeaf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: dlt\n",
      "Version: 1.12.3\n",
      "Summary: dlt is an open-source python-first scalable data loading library that does not require any backend to run.\n",
      "Home-page: https://github.com/dlt-hub\n",
      "Author: \n",
      "Author-email: \"dltHub Inc.\" <services@dlthub.com>\n",
      "License-Expression: Apache-2.0\n",
      "Location: /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages\n",
      "Requires: click, fsspec, gitpython, giturlparse, hexbytes, humanize, jsonpath-ng, orjson, packaging, pathvalidate, pendulum, pluggy, pytz, pyyaml, requests, requirements-parser, rich-argparse, semver, setuptools, simplejson, sqlglot, tenacity, tomlkit, typing-extensions, tzdata\n",
      "Required-by: \n",
      "dlt                                      1.12.3\n",
      "qdrant-client                            1.14.2\n"
     ]
    }
   ],
   "source": [
    "!pip show dlt\n",
    "!pip list | grep -E \"dlt|qdrant\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23e988",
   "metadata": {},
   "source": [
    "### dlt use\n",
    "\n",
    "#### Use dlt to wrap this flow\n",
    "\n",
    "`dlt helps you:`\n",
    "\n",
    "- Make this flow reproducible\n",
    "\n",
    "- Track state and logs\n",
    "\n",
    "- Potentially scale or schedule it\n",
    "\n",
    "`In dlt, your job is to define:`\n",
    "\n",
    "- A source → gives you the data\n",
    "\n",
    "- A resource → prepares and formats it\n",
    "\n",
    "- A pipeline → runs it and optionally loads it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a088f",
   "metadata": {},
   "source": [
    "### dlt Resourse\n",
    "\n",
    "A `resource` is an (optionally async) function that yields data. To create a resource, we add the `@dlt.resource` decorator to that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c473c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dlt\n",
    "\n",
    "@dlt.resource\n",
    "def zoomcamp_data():\n",
    "    docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "    docs_response = requests.get(docs_url)\n",
    "    documents_raw = docs_response.json()\n",
    "\n",
    "    for course in documents_raw:\n",
    "        course_name = course['course']\n",
    "\n",
    "        for doc in course['documents']:\n",
    "            doc['course'] = course_name\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36819087",
   "metadata": {},
   "source": [
    "### Question 2. dlt pipeline\n",
    "\n",
    "Now let's create a pipeline.\n",
    "\n",
    "We need to define a destination for that. Let's use the qdrant one:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296ea56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create a folder with our data, and the name for it will be db.qdrant\n",
    "from dlt.destinations import qdrant\n",
    "\n",
    "qdrant_destination = qdrant(\n",
    "  qd_path=\"db.qdrant\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2a1c3",
   "metadata": {},
   "source": [
    "### pipeline\n",
    "\n",
    "A `pipeline` moves data from your Python code to a destination. The pipeline accepts `dlt` sources or resources, as well as generators, async generators, lists, and any iterables. Once the pipeline runs, all resources are evaluated and the data is loaded at the destination.\n",
    "\n",
    "##### You instantiate a pipeline by calling the `dlt.pipeline` function with the following arguments:\n",
    "\n",
    "`pipeline_name`: a name of the pipeline that will be used to identify it in trace and monitoring events and to restore its state and data schemas on subsequent runs. If not provided, dlt will create a pipeline name from the file name of the currently executing Python module.\n",
    "\n",
    "`destination`: a name of the `destination` to which dlt will load the data. It may also be provided to the `run` method of the pipeline.\n",
    "\n",
    "`dataset_name`: a name of the dataset to which the data will be loaded.\n",
    "A dataset is a logical group of tables, i.e., schema in relational databases or a folder grouping many files. It may also be provided later to the `run` or `load` methods of the pipeline. If not provided, then it defaults to the `{pipeline_name}_dataset` on destinations that require datasets (most of the warehouses). It will stay empty on destinations that do not separate tables into datasets (or database schemas) ie. on vector databases or Clikchouse.\n",
    "\n",
    "> To `load` the data, you call the `run` method and pass your data in the data argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76944232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a522fecc725141d5bd91e398bd1f1c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770cf32a307f43858ae52e521bdaceab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7786e165784e48cf859924269fd51895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd29e289fdb47a28e817599c0ac0d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd12e64d68e4004a1e89399b3f3fa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898762edee714ee69adfc12fce9f8323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started at 2025-07-07 01:29:41.726168+00:00 and COMPLETED in 16.21 seconds with 4 steps.\n",
      "Step extract COMPLETED in 0.95 seconds.\n",
      "\n",
      "Load package 1751851790.558231 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step normalize COMPLETED in 0.13 seconds.\n",
      "Normalized data for the following tables:\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "- zoomcamp_data: 948 row(s)\n",
      "\n",
      "Load package 1751851790.558231 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "\n",
      "Step load COMPLETED in 6.32 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 6.30 seconds\n",
      "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
      "The qdrant destination used /Users/selamsew/Documents/llm-zoomcampl/workshop/db.qdrant location to store data\n",
      "Load package 1751851790.558231 is LOADED and contains no failed jobs\n",
      "\n",
      "Step run COMPLETED in 16.21 seconds.\n",
      "Pipeline zoomcamp_pipeline load step completed in 6.30 seconds\n",
      "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
      "The qdrant destination used /Users/selamsew/Documents/llm-zoomcampl/workshop/db.qdrant location to store data\n",
      "Load package 1751851790.558231 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"zoomcamp_pipeline\",\n",
    "    destination=qdrant_destination,\n",
    "    dataset_name=\"zoomcamp_tagged_data\"\n",
    "\n",
    ")\n",
    "load_info = pipeline.run(zoomcamp_data())\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85239d",
   "metadata": {},
   "source": [
    "### Question 2. dlt pipeline:  Answer -> 948\n",
    "\n",
    "Normalized data for the following tables:\n",
    "\n",
    "- _dlt_pipeline_state: 1 row(s)\n",
    "- zoomcamp_data: 948 row(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a2ba5",
   "metadata": {},
   "source": [
    "### Question 3. Embeddings\n",
    "\n",
    "#### When inserting the data, an embedding model was used. Which one?\n",
    "\n",
    "`mata.json` -> information about db.qdrant, it used `\"fast-bge-small-en\"`\n",
    "\n",
    "here a some part of `mata.json`\n",
    "\n",
    "```json\n",
    "\"collections\": {\n",
    "        \"zoomcamp_tagged_data\": {\n",
    "            \"vectors\": {\n",
    "                \"fast-bge-small-en\": {\n",
    "                    \"size\": 384,\n",
    "                    \"distance\": \"Cosine\",\n",
    "                    \"hnsw_config\": null,\n",
    "                    \"quantization_config\": null,\n",
    "                    \"on_disk\": null,\n",
    "                    \"datatype\": null,\n",
    "                    \"multivector_config\": null\n",
    "    }\n",
    "}       \n",
    "```\n",
    "\n",
    "\n",
    "### Answer: `fast-bge-small-en`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
