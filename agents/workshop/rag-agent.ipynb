{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c3af91e",
      "metadata": {},
      "source": [
        "# In this workshop we\n",
        "\n",
        "- Build a RAG application on the FAQ database\n",
        "- Make it agentic\n",
        "- Learn about agentic search\n",
        "- Give tools to our agents\n",
        "- Use PydanticAI to make it easier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99f515a6",
      "metadata": {},
      "source": [
        "#### first lets install minsearch which we will turn it into agentic search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b69d0163",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install jupyter openai minsearch requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0a4f4d",
      "metadata": {},
      "source": [
        "### Basic RAG\n",
        "\n",
        "RAG consisits of 3 parts\n",
        "\n",
        "- Search\n",
        "- prompt\n",
        "- LLM\n",
        "\n",
        "``` python\n",
        "def rag(query):\n",
        "    search_results = search(query)\n",
        "    prompt = build_prompt(query, search_results)\n",
        "    answer = llm(prompt)\n",
        "    return answer\n",
        "```\n",
        "query -> get top k -> build prompt (using top k and other requirments as contexts) -> feed that to llm -> final search result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d24b6ff0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import requests \n",
        "from minsearch import AppendableIndex\n",
        "\n",
        "# get the documents \n",
        "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
        "docs_response = requests.get(docs_url)\n",
        "documents_raw = docs_response.json()\n",
        "\n",
        "documents = []\n",
        "\n",
        "for course in documents_raw:\n",
        "    course_name = course['course']\n",
        "\n",
        "    for doc in course['documents']:\n",
        "        doc['course'] = course_name\n",
        "        documents.append(doc)\n",
        "        \n",
        "\n",
        "# index them\n",
        "index = AppendableIndex(\n",
        "    text_fields=[\"question\", \"text\", \"section\"],\n",
        "    keyword_fields=[\"course\"]\n",
        ")\n",
        "\n",
        "index.fit(documents)    \n",
        "\n",
        "# build the search function\n",
        "\n",
        "def search(query):\n",
        "    \n",
        "    boost = {'question': 3.0, 'section': 0.5}\n",
        "    results = index.search(\n",
        "        query=query,\n",
        "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
        "        boost_dict=boost,\n",
        "        num_results=5,\n",
        "        output_ids=True\n",
        "    )\n",
        "\n",
        "    return results    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ced0a51d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# building the prompt\n",
        "prompt_template = \"\"\"\n",
        "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
        "Use only the facts from the CONTEXT when answering the QUESTION.\n",
        "\n",
        "<QUESTION>\n",
        "{question}\n",
        "</QUESTION>\n",
        "\n",
        "<CONTEXT>\n",
        "{context}\n",
        "</CONTEXT>\n",
        "\"\"\".strip()\n",
        "\n",
        "def build_prompt(query, search_results):\n",
        "    context = \"\"\n",
        "\n",
        "    for doc in search_results:\n",
        "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
        "    \n",
        "    prompt = prompt_template.format(question=query, context=context).strip()\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "32f7dd2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get our llm ready\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def llm(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-3.5-turbo-1106',\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def rag(query):\n",
        "    search_results = search(query)\n",
        "    prompt = build_prompt(query, search_results)\n",
        "    answer = llm(prompt)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbc72567",
      "metadata": {},
      "source": [
        "##### Now that we have the RAG, let make it agentic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3195e122",
      "metadata": {},
      "source": [
        "### Part 1: Agentic RAG\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
