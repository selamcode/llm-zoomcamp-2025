{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2120ce06",
   "metadata": {},
   "source": [
    "### Load documents with IDs\n",
    "\n",
    "- Loads a JSON file (`documents-with-ids.json`) containing documents with unique IDs used for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd0b2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/search_evaluation/'\n",
    "\n",
    "docs_url = url_prefix + 'documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3451cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': 'ea739c65'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of the orginal FAQ\n",
    "\n",
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8260815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At id 'c02e79ef' we have\n",
      "The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  ‚ÄúOffice Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don‚Äôt forget to register in DataTalks.Club's Slack and join the channel.\n"
     ]
    }
   ],
   "source": [
    "# lets create a map (dict) id to text\n",
    "doc_idx = {d['id']: d for d in documents}\n",
    "\n",
    "# here text is the answers to the questions\n",
    "sample_data = doc_idx['c02e79ef']['text']\n",
    "print(f'At id \\'c02e79ef\\' we have\\n{sample_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55857e53",
   "metadata": {},
   "source": [
    "> now that we the data we want, it needs to be `indexed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687bbbd",
   "metadata": {},
   "source": [
    "### Index Data\n",
    "\n",
    "#### Indexing:\n",
    "\n",
    "Is the process of storing data in a structured way that allows for fast and efficient retrieval.\n",
    "\n",
    "So let's use `elasticsearch` here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83cb96",
   "metadata": {},
   "source": [
    "Step 1: Pick a Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca09f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04a752",
   "metadata": {},
   "source": [
    "### What did we do in this step?\n",
    "\n",
    "#### Picked a Transformer Model\n",
    "\n",
    "- This is a bi-encoder model trained for semantic similarity (e.g., question‚Äìanswer retrieval)\n",
    "- It turns input text into a 384-dimensional vector.\n",
    "- You chose this so that instead of using exact words, you can compare meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387d78a",
   "metadata": {},
   "source": [
    "### Step 2: Connect to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71e77891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3a02bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"question_text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# setting the name of the Elasticsearch index\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "# This deletes the existing index named \"course-questions\" if it already exists.\n",
    "# for the purpose of index exist error, we can find a better way of dealing with this\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "\n",
    "# This creates a new index named \"course-questions\" using the settings and mappings defined in index_settings.\n",
    "es_client.indices.create(index=index_name, body=index_settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54034977",
   "metadata": {},
   "source": [
    "### Step 3: Define and Create the Index\n",
    "\n",
    "#### Think of the index like this:\n",
    "\n",
    "> An index in Elasticsearch is like a database table that's been created with a structure (`settings` + `mappings`) but no data yet.\n",
    "\n",
    "- At creation, it‚Äôs empty, just a ‚Äúcontainer‚Äù with rules.\n",
    "- You can then send documents into it, one by one or in bulk.\n",
    "- As long as each document matches the mapping rules (e.g., `question_text_vector` must be a list of 384 floats), it will be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d13156",
   "metadata": {},
   "source": [
    "### Now lets prepare and send out data to our index (database)\n",
    "\n",
    "- for prgress bar we will use the `tqdm`\n",
    "    - `tqdm.auto` automatically decides the best way to show the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e51147a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c546a277d04b1c965bc63f641320c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    question = doc['question']\n",
    "    text = doc['text']\n",
    "    doc['question_text_vector'] = model.encode(question + ' ' + text)\n",
    "\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b56952",
   "metadata": {},
   "source": [
    "- You take the question and text fields,\n",
    "\n",
    "- Concatenate them,\n",
    "\n",
    "- Encode the result using your transformer model, producing a vector,\n",
    "\n",
    "- Store that vector inside the doc under the question_text_vector key.\n",
    "\n",
    "- You send this updated document to Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d31cfd",
   "metadata": {},
   "source": [
    "> Now that we have our knowledge DB, it's time to perforem different actions on it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc666c",
   "metadata": {},
   "source": [
    "### Retrieval \n",
    "- searching in the knowledge db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64fc9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# field: the vector field to search in (e.g., 'question_text_vector')\n",
    "# vector: the query vector you‚Äôre searching for\n",
    "# course: filters results only to this course (e.g., \"data-engineering\")\n",
    "\n",
    "def elastic_search_knn(field, vector, course):\n",
    "\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5, # top 5 \n",
    "        \"num_candidates\": 10000, # Elasticsearch looks at up to 10,000 docs to find the best 5 (k). This improves quality.\n",
    "        # filter: only search documents that belong to the\n",
    "        \"filter\": {\n",
    "            \"term\": {\n",
    "                \"course\": course\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # _source: only return these fields in the result (not everything), to keep it clean.\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"text\", \"section\", \"question\", \"course\", \"id\"]\n",
    "    }\n",
    "\n",
    "    # Runs the search in Elasticsearch using the query we just built.\n",
    "    es_results = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query\n",
    "    )\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    \n",
    "    # ['hits']['hits'] is a list of  list of individual matched documents.\n",
    "    for hit in es_results['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "# prepare your query\n",
    "def question_text_vector_knn(q):\n",
    "    \n",
    "    # get the question text and course from the input q\n",
    "    question = q['question']\n",
    "    course = q['course']\n",
    "\n",
    "    \n",
    "    # use your transformer model to turn the question into a vector\n",
    "    v_q = model.encode(question)\n",
    "\n",
    "    return elastic_search_knn('question_text_vector', v_q, course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa8505",
   "metadata": {},
   "source": [
    "### knn (k-nearest neighbors)\n",
    "\n",
    "`knn` - In Elasticsearch, starting from version 8.0+, there's built-in support for k-NN (k-nearest neighbors) search on dense vectors.\n",
    "\n",
    "```json\n",
    "\"knn\": {\n",
    "    \"field\": \"question_text_vector\",\n",
    "    \"query_vector\": [...],\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 10000,\n",
    "    \"filter\": {\n",
    "        \"term\": {\"course\": \"search101\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "this is Elasticsearch-specific syntax for dense vector search (using their built-in k-NN engine like HNSW).\n",
    "\n",
    "So:\n",
    "\n",
    "- `field` tells Elasticsearch which vector field to compare.\n",
    "\n",
    "- `query_vector` is the encoded input vector.\n",
    "\n",
    "- `k` is how many similar results you want.\n",
    "\n",
    "- `num_candidates` affects performance/quality.\n",
    "\n",
    "- `filter` lets you limit the search (e.g., by course).\n",
    "\n",
    "\n",
    "### What is `['hits']['hits']`?\n",
    "\n",
    "When you run a search in Elasticsearch, the response is a nested JSON object. It looks something like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"hits\": {\n",
    "    \"total\": 123,\n",
    "    \"hits\": [\n",
    "      {\"_source\": {...}},  // 1st result\n",
    "      {\"_source\": {...}},  // 2nd result\n",
    "      ...\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "So:\n",
    "\n",
    "- `es_results['hits']` ‚Üí gives you the whole section of search results.\n",
    "\n",
    "- `es_results['hits']['hits']` ‚Üí gives you just the list of individual matched documents.\n",
    "\n",
    "Then inside each hit, the actual document is found under `['_source']`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1559fd",
   "metadata": {},
   "source": [
    "### üß† Important point\n",
    "\n",
    "- The `model` is not doing the search.\n",
    "\n",
    "- `knn` is not doing the embedding.\n",
    "\n",
    "They‚Äôre separate, but used together:\n",
    "\n",
    "   -  `Model` = turns input ‚Üí vector.\n",
    "\n",
    "   -  `k-NN` = finds vectors most similar to it\n",
    "\n",
    "\n",
    "\n",
    "##### Now that we have our knowledge DB and the way we do search on it (`knn`), let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d889e037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Everything is recorded, so you won‚Äôt miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'What if I miss a session?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5170565b'},\n",
       " {'text': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Is it going to be live? When?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '39fda9f0'},\n",
       " {'text': '(Hrithik Kumar Advani)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Useful Resource for Missing Data Treatment\\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '81b8e8d0'},\n",
       " {'text': \"Problem description\\nThe accuracy and the loss are both still the same or nearly the same while training.\\nSolution description\\nIn the homework, you should set class_mode='binary' while reading the data.\\nAlso, problem occurs when you choose the wrong optimizer, batch size, or learning rate\\nAdded by Ekaterina Kutovaia\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'The same accuracy on epochs',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7d11d5ce'},\n",
       " {'text': \"Yes, it's possible. See the previous answer.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Will I get a certificate if I missed the midterm project?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1d644223'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_text_vector_knn(dict(\n",
    "    question='Are sessions recorded if I miss one?',\n",
    "    course='machine-learning-zoomcamp'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac80fd",
   "metadata": {},
   "source": [
    "#### Now we have retrival working let go the next step. using the llm to give smarter answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5325b4",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05743da5",
   "metadata": {},
   "source": [
    "### The RAG Flow\n",
    "\n",
    "> Take a question ‚Üí find related answers (knn search) ‚Üí feed both to GPT (llm) ‚Üí get a smart, grounded response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c4823",
   "metadata": {},
   "source": [
    "#### Let prepare the `prompt` for our llm model. This will shape the answer we get at the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "958fab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: the user‚Äôs question (a string)\n",
    "# search_results: a list of documents returned from Elasticsearch\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip() # strip to remove a place whitespaces at the start and end of a string\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    # from the list of documents we get from Elasticsearch (after knn), get the secion question and text fields\n",
    "    # so we can use them as a context\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    # format the prompt, the .format will help as insert the variables in the {}\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325af770",
   "metadata": {},
   "source": [
    "We buidl the promp function which gives us a nicely formated formated query for the llm. Now lets get do the communication with LLM (chatgpt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec03131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-3.5-turbo-1106'):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    print\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5bef0",
   "metadata": {},
   "source": [
    "#### now lets investigate the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05977824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The type of reponse:\n",
      "\n",
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "\n",
      "Our repsonse object looks like this:\n",
      "\n",
      "{'id': 'chatcmpl-BtPw0tpCrk0uk9sO6OoIqB7YD9eD6', 'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Hello! How can I assist you today?', 'refusal': None, 'role': 'assistant', 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}}], 'created': 1752547432, 'model': 'gpt-3.5-turbo-1106', 'object': 'chat.completion', 'service_tier': 'default', 'system_fingerprint': 'fp_982035f36f', 'usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}\n",
      "\n",
      "This goes in the object tree of datastructures get what we need, which is the chat reponse:\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "response = client.chat.completions.create(model='gpt-3.5-turbo-1106', messages=[{\"role\": \"user\", \"content\": \"hi\"}])\n",
    "\n",
    "print(\"\\nThe type of reponse:\\n\")\n",
    "print(type(response))\n",
    "\n",
    "print(\"\\nOur repsonse object looks like this:\\n\")\n",
    "print(response.model_dump())\n",
    "\n",
    "print(\"\\nThis goes in the object tree of datastructures get what we need, which is the chat reponse:\\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac844a",
   "metadata": {},
   "source": [
    "### How we accessed the `<class 'openai.types.chat.chat_completion.ChatCompletion'>` object repsonse \n",
    "```\n",
    "response\n",
    "‚îú‚îÄ‚îÄ choices (list)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ [0]|\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ message (dict)\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ content ‚Üí üü© \"Hello! How can I assist you today?\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07dec3",
   "metadata": {},
   "source": [
    "### role field\n",
    "- `system` and `user`\n",
    "\n",
    "`\"system\"`: Tells the assistant how to answer (the behavior or style). (we are telling it in our prompt, but we could also set that using the system role).\n",
    "\n",
    "`\"user\"`: Gives the assistant what to answer (the question or input).\n",
    "\n",
    "- The assistant replies based on both the system‚Äôs instructions and the user‚Äôs input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cefd04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "client = OpenAI()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly assistant who replies with short answers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=messages\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6855a5",
   "metadata": {},
   "source": [
    "### now we ahve the `search using knn` and `llm response` functions, the only missing is a function that combines these two and provide a final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdc52e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query: dict, model=\"gpt-3.5-turbo-1106\") -> str:\n",
    "    \n",
    "    # get top k answers\n",
    "    search_results = question_text_vector_knn(query)\n",
    "    \n",
    "    # build prompt, using question and search result\n",
    "    prompt = build_prompt(query['question'], search_results)\n",
    "    \n",
    "    # feed llm with prompt, and choose the model\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bcde8",
   "metadata": {},
   "source": [
    "Now lets check our `rag` function (rag search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4a7b04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Can I enroll in the course after it starts?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'document': '7842b56a'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa589b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, you can still enroll in the course after it starts. There will be deadlines for turning in the final projects, so it's recommended not to leave everything for the last minute. Additionally, all the materials will be kept after the course finishes, so you can follow the course at your own pace after it finishes.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(ground_truth[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91ce14",
   "metadata": {},
   "source": [
    "### Now we have a workinf RAG system, it's time to evauate using evaluating metrics\n",
    "\n",
    "#### offline-evaluation 1 :  Cosine Similarity Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc77cea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
