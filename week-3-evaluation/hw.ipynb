{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2655f5a7",
      "metadata": {},
      "source": [
        "# Homework: Search Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "84fb77fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall minsearch -y\n",
        "!pip install -U minsearch qdrant_client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ccbc36",
      "metadata": {},
      "source": [
        "### Evaluation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dd36b338",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
        "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
        "documents = requests.get(docs_url).json()\n",
        "\n",
        "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
        "df_ground_truth = pd.read_csv(ground_truth_url)\n",
        "ground_truth = df_ground_truth.to_dict(orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff85174f",
      "metadata": {},
      "source": [
        "Here, `documents` contains the documents from the FAQ database with unique IDs, and `ground_truth` contains generated question-answer pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf350e1",
      "metadata": {},
      "source": [
        "### we will need the following code for evaluating retrieval: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "04188666",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Measures if at least one relevant document is found in the top k results.\n",
        "def hit_rate(relevance_total):\n",
        "    cnt = 0\n",
        "\n",
        "    for line in relevance_total:\n",
        "        if True in line:\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    return cnt / len(relevance_total)\n",
        "\n",
        "#  Average rank of the first relevant document across queries.\n",
        "def mrr(relevance_total):\n",
        "    total_score = 0.0\n",
        "\n",
        "    for line in relevance_total:\n",
        "        for rank in range(len(line)):\n",
        "            if line[rank] == True:\n",
        "                total_score = total_score + 1 / (rank + 1)\n",
        "\n",
        "    return total_score / len(relevance_total)\n",
        "\n",
        "def evaluate(ground_truth, search_function):\n",
        "    relevance_total = []\n",
        "\n",
        "    for q in tqdm(ground_truth):\n",
        "        doc_id = q['document']\n",
        "        results = search_function(q)\n",
        "        relevance = [d['id'] == doc_id for d in results]\n",
        "        relevance_total.append(relevance)\n",
        "\n",
        "    return {\n",
        "        'hit_rate': hit_rate(relevance_total),\n",
        "        'mrr': mrr(relevance_total),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0745878",
      "metadata": {},
      "source": [
        "### Q1. Minsearch text\n",
        "\n",
        "Now let's evaluate our usual minsearch approach, but tweak the parameters. Let's use the following boosting params:\n",
        "\n",
        "```python\n",
        " boost = {'question': 1.5, 'section': 0.1} \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "65e4c069",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<minsearch.minsearch.Index at 0x7f88a103d820>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from minsearch import Index\n",
        "\n",
        "boost = {'question': 1.5, 'section': 0.1}\n",
        "\n",
        "# initalize our index\n",
        "index = Index(\n",
        "    text_fields=['question', 'text', 'section'],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "index.fit(documents) # making out document indexable \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2dd3a17c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total queries: 4627\n",
            "Example query: {'question': 'When does the course begin?', 'course': 'data-engineering-zoomcamp', 'document': 'c02e79ef'}\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total queries: {len(ground_truth)}\")\n",
        "print(\"Example query:\", ground_truth[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ff9007ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# search function for a certain query\n",
        "\n",
        "def search_function(q):\n",
        "    return index.search(\n",
        "        query=q['question'],\n",
        "        filter_dict=None,\n",
        "        boost_dict=boost,\n",
        "        num_results=10\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d130e58e",
      "metadata": {},
      "source": [
        "### Now we will feed each question from `ground_truth` to our `search_function` (minsearch), then we will compare the result from the search to the ground_truth answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "284a8f90",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbf03cc936564351a52eb6a29740e108",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hit_rate': 0.8597363302355738, 'mrr': 0.6897542375497872}\n"
          ]
        }
      ],
      "source": [
        "metrics = evaluate(ground_truth, search_function)\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c02e40",
      "metadata": {},
      "source": [
        "# `Q1-Answer -> 0.85 and the closer answer is 0.84` "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d15dfa6",
      "metadata": {},
      "source": [
        "### Embeddings\n",
        "\n",
        "The latest version of minsearch also supports vector search. We will use it:\n",
        "\n",
        "\n",
        "We will also use `TF-IDF (Term Frequency â€“ Inverse Document Frequency)` and Singular Value Decomposition to create embeddings from texts.\n",
        "\n",
        "#### What TF-IDF Does:\n",
        "\n",
        "It looks at word appearance patterns across the documents.\n",
        "\n",
        "It gives more weight to:\n",
        "\n",
        "- Words that appear frequently in a specific document (high term frequency),\n",
        "\n",
        "- But less frequently across all documents (high inverse document frequency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ed04b510",
      "metadata": {},
      "outputs": [],
      "source": [
        "from minsearch import VectorSearch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3485e329",
      "metadata": {},
      "source": [
        "#### Let's create embeddings for the \"question\" field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ba0f8ea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = []\n",
        "\n",
        "for doc in documents:\n",
        "    t = doc['question']\n",
        "    texts.append(t)\n",
        "    \n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(min_df=3), # Only keep words that appear in at least 3 questions (removes noise/rare words).\n",
        "    \n",
        "    # we use random_state for repeatable results (for testing, debugging, or sharing).\n",
        "    TruncatedSVD(n_components=128, random_state=1) # 128 dimensions, and Hey computer, random_state use the same random choices every time\n",
        ")\n",
        "\n",
        "# Creates a reusable pipeline\n",
        "X = pipeline.fit_transform(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e08a10d",
      "metadata": {},
      "source": [
        "### Q2. Vector search for question\n",
        "\n",
        "Now let's index these embeddings with minsearch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27d583d4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<minsearch.vector.VectorSearch at 0x7f88a12d39a0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vindex = VectorSearch(keyword_fields={'course'})\n",
        "vindex.fit(X, documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee83053",
      "metadata": {},
      "source": [
        "#### create the `search_function`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "309e241a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def search_function(q):\n",
        "    query_vec = pipeline.transform([q['question']])\n",
        "    return vindex.search(query_vec, filter_dict=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0cf588",
      "metadata": {},
      "source": [
        "#### Now let's evaluate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1256df5f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2347fc6f524a0cb1da3896361f8fd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hit_rate': 0.4696347525394424, 'mrr': 0.30031389257669755}\n"
          ]
        }
      ],
      "source": [
        "metrics = evaluate(ground_truth, search_function)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19a58d8",
      "metadata": {},
      "source": [
        "# `Q2-Answer -> mrr': 0.3, so close one is 0.35`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e6f61f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "37cdd02e",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
