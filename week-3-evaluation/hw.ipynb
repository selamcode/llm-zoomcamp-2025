{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2655f5a7",
      "metadata": {},
      "source": [
        "# Homework: Search Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "84fb77fe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: minsearch 0.0.4\n",
            "Uninstalling minsearch-0.0.4:\n",
            "  Successfully uninstalled minsearch-0.0.4\n",
            "Collecting minsearch\n",
            "  Using cached minsearch-0.0.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: qdrant_client in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (1.15.0)\n",
            "Requirement already satisfied: numpy in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from minsearch) (1.24.4)\n",
            "Requirement already satisfied: pandas in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from minsearch) (2.3.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from minsearch) (1.6.1)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from qdrant_client) (1.71.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.28.1)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from qdrant_client) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from qdrant_client) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from qdrant_client) (2.11.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from qdrant_client) (2.5.0)\n",
            "Requirement already satisfied: anyio in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.2.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.4.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from pandas->minsearch) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from pandas->minsearch) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from pandas->minsearch) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->minsearch) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->minsearch) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->minsearch) (3.6.0)\n",
            "Using cached minsearch-0.0.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: minsearch\n",
            "Successfully installed minsearch-0.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall minsearch -y\n",
        "!pip install -U minsearch qdrant_client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ccbc36",
      "metadata": {},
      "source": [
        "### Evaluation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dd36b338",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
        "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
        "documents = requests.get(docs_url).json()\n",
        "\n",
        "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
        "df_ground_truth = pd.read_csv(ground_truth_url)\n",
        "ground_truth = df_ground_truth.to_dict(orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff85174f",
      "metadata": {},
      "source": [
        "Here, `documents` contains the documents from the FAQ database with unique IDs, and `ground_truth` contains generated question-answer pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf350e1",
      "metadata": {},
      "source": [
        "### we will need the following code for evaluating retrieval: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "04188666",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Measures if at least one relevant document is found in the top k results.\n",
        "def hit_rate(relevance_total):\n",
        "    cnt = 0\n",
        "\n",
        "    for line in relevance_total:\n",
        "        if True in line:\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    return cnt / len(relevance_total)\n",
        "\n",
        "#  Average rank of the first relevant document across queries.\n",
        "def mrr(relevance_total):\n",
        "    total_score = 0.0\n",
        "\n",
        "    for line in relevance_total:\n",
        "        for rank in range(len(line)):\n",
        "            if line[rank] == True:\n",
        "                total_score = total_score + 1 / (rank + 1)\n",
        "\n",
        "    return total_score / len(relevance_total)\n",
        "\n",
        "def evaluate(ground_truth, search_function):\n",
        "    relevance_total = []\n",
        "\n",
        "    for q in tqdm(ground_truth):\n",
        "        doc_id = q['document']\n",
        "        results = search_function(q)\n",
        "        relevance = [d['id'] == doc_id for d in results]\n",
        "        relevance_total.append(relevance)\n",
        "\n",
        "    return {\n",
        "        'hit_rate': hit_rate(relevance_total),\n",
        "        'mrr': mrr(relevance_total),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0745878",
      "metadata": {},
      "source": [
        "### Q1. Minsearch text\n",
        "\n",
        "Now let's evaluate our usual minsearch approach, but tweak the parameters. Let's use the following boosting params:\n",
        "\n",
        "```python\n",
        " boost = {'question': 1.5, 'section': 0.1} \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "65e4c069",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<minsearch.minsearch.Index at 0x7f889fb16430>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from minsearch import Index\n",
        "\n",
        "boost = {'question': 1.5, 'section': 0.1}\n",
        "\n",
        "# initalize our index\n",
        "index = Index(\n",
        "    text_fields=['question', 'text', 'section'],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "index.fit(documents) # making out document indexable \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2dd3a17c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total queries: 4627\n",
            "Example query: {'question': 'When does the course begin?', 'course': 'data-engineering-zoomcamp', 'document': 'c02e79ef'}\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total queries: {len(ground_truth)}\")\n",
        "print(\"Example query:\", ground_truth[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ff9007ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# search function for a certain query\n",
        "\n",
        "def search_function(q):\n",
        "    return index.search(\n",
        "        query=q['question'],\n",
        "        filter_dict=None,\n",
        "        boost_dict=boost,\n",
        "        num_results=10\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d130e58e",
      "metadata": {},
      "source": [
        "### Now we will feed each question from `ground_truth` to our `search_function` (minsearch), then we will compare the result from the search to the ground_truth answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "284a8f90",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af89799590044be8a04df0c6d8c93219",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hit_rate': 0.8597363302355738, 'mrr': 0.6897542375497872}\n"
          ]
        }
      ],
      "source": [
        "metrics = evaluate(ground_truth, search_function)\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c02e40",
      "metadata": {},
      "source": [
        "# `Q1-Answer -> 0.85 and the closer answer is 0.84` "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d15dfa6",
      "metadata": {},
      "source": [
        "### Embeddings\n",
        "\n",
        "The latest version of minsearch also supports vector search. We will use it:\n",
        "\n",
        "\n",
        "We will also use `TF-IDF (Term Frequency – Inverse Document Frequency)` and Singular Value Decomposition to create embeddings from texts.\n",
        "\n",
        "#### What TF-IDF Does:\n",
        "\n",
        "It looks at word appearance patterns across the documents.\n",
        "\n",
        "It gives more weight to:\n",
        "\n",
        "- Words that appear frequently in a specific document (high term frequency),\n",
        "\n",
        "- But less frequently across all documents (high inverse document frequency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ed04b510",
      "metadata": {},
      "outputs": [],
      "source": [
        "from minsearch import VectorSearch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3485e329",
      "metadata": {},
      "source": [
        "#### Let's create embeddings for the \"question\" field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ba0f8ea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = []\n",
        "\n",
        "for doc in documents:\n",
        "    t = doc['question']\n",
        "    texts.append(t)\n",
        "    \n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(min_df=3), # Only keep words that appear in at least 3 questions (removes noise/rare words).\n",
        "    \n",
        "    # we use random_state for repeatable results (for testing, debugging, or sharing).\n",
        "    TruncatedSVD(n_components=128, random_state=1) # 128 dimensions, and Hey computer, random_state use the same random choices every time\n",
        ")\n",
        "\n",
        "# Creates a reusable pipeline\n",
        "X = pipeline.fit_transform(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e08a10d",
      "metadata": {},
      "source": [
        "### Q2. Vector search for question\n",
        "\n",
        "Now let's index these embeddings with minsearch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "27d583d4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<minsearch.vector.VectorSearch at 0x7f889fb16a00>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vindex = VectorSearch(keyword_fields={'course'})\n",
        "vindex.fit(X, documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee83053",
      "metadata": {},
      "source": [
        "#### create the `search_function`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "309e241a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def search_function(q):\n",
        "    query_vec = pipeline.transform([q['question']])\n",
        "    return vindex.search(query_vec, filter_dict=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0cf588",
      "metadata": {},
      "source": [
        "#### Now let's evaluate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1256df5f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f13aee90ea34a568d7da8198fc1d393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hit_rate': 0.4696347525394424, 'mrr': 0.30031389257669755}\n"
          ]
        }
      ],
      "source": [
        "metrics = evaluate(ground_truth, search_function)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19a58d8",
      "metadata": {},
      "source": [
        "# `Q2-Answer -> mrr': 0.3, so close one is 0.35`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e6f61f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "434f96ea455e4ba0b66b5308061d6fc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hit_rate': 0.8415820185865571, 'mrr': 0.6254320739894556}\n"
          ]
        }
      ],
      "source": [
        "# Create the pipeline and fit it\n",
        "texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(min_df=3),\n",
        "    TruncatedSVD(n_components=128, random_state=1)\n",
        ")\n",
        "\n",
        "Y = pipeline.fit_transform(texts)\n",
        "\n",
        "# Create and fit the index\n",
        "vindex_qa = VectorSearch(keyword_fields={'course'})\n",
        "vindex_qa.fit(Y, documents)\n",
        "\n",
        "# search\n",
        "def search_function(q):\n",
        "    query_vec = pipeline.transform([q['question']])\n",
        "    return vindex_qa.search(query_vec, filter_dict=None)\n",
        "\n",
        "# Now evaluate\n",
        "metrics = evaluate(ground_truth, search_function)\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f416ec65",
      "metadata": {},
      "source": [
        "# `Q3-Answer -> hit_rat : 0.84, so close one is 0.82`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d58add",
      "metadata": {},
      "source": [
        "### Q4. Qdrant\n",
        "\n",
        "Now let's evaluate the following settings in Qdrant:\n",
        "\n",
        "text = doc['question'] + ' ' + doc['text']\n",
        "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
        "limit = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b146a104",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e2fd1cd0c7b4919b7de53d3e7b9ab01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hit_rate': 0.8415820185865571, 'mrr': 0.6254320739894556}\n"
          ]
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer(\"jinaai/jina-embeddings-v2-small-en\")\n",
        "\n",
        "# Initialize client\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Create the collection only if it doesn't already exist\n",
        "if not client.collection_exists(\"qa_eval\"):\n",
        "    client.create_collection(\n",
        "        collection_name=\"qa_eval\",\n",
        "        vectors_config=VectorParams(size=512, distance=Distance.COSINE)\n",
        "    )\n",
        "\n",
        "# Index documents\n",
        "for i, doc in enumerate(documents):\n",
        "    text = doc['question'] + ' ' + doc['text']\n",
        "    vec = model.encode(text).tolist()\n",
        "    client.upsert(\n",
        "        collection_name=\"qa_eval\",\n",
        "        points=[PointStruct(id=i, vector=vec, payload={\"id\": i})]\n",
        "    )\n",
        "\n",
        "# Define search function\n",
        "def qdrant_search(doc, k=5):\n",
        "    query = doc['question'] + ' ' + doc['text']\n",
        "    qvec = model.encode(query).tolist()\n",
        "\n",
        "    hits = client.search(\n",
        "        collection_name=\"qa_eval\",\n",
        "        query_vector=qvec,\n",
        "        limit=k\n",
        "    )\n",
        "    return [hit.payload['id'] for hit in hits]\n",
        "# Evaluate\n",
        "metrics = evaluate(ground_truth, search_function)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37cdd02e",
      "metadata": {},
      "source": [
        "# `Q4-Answer -> mrr : 0.625 ~ 0.63, so close one from the choice is 0.65`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa5e81d",
      "metadata": {},
      "source": [
        "# Q5. Cosine simiarity\n",
        "\n",
        "In the second part of the module, we looked at evaluating the entire RAG approach. In particular, we looked at comparing the answer generated by our system with the actual answer from the FAQ.\n",
        "\n",
        "One of the ways of doing it is using the cosine similarity. Let's see how to calculate it.\n",
        "\n",
        "Cosine similarity is a dot product between two normalized vectors. In geometrical sense, it's the cosine of the angle between the vectors. Look up \"cosine similarity geometry\" if you want to learn more about it.\n",
        "\n",
        "For us, it means that we need two things:\n",
        "\n",
        "- First, we normalize each of the vectors\n",
        "- Then, compute the dot product\n",
        "\n",
        "So, we get this:\n",
        "\n",
        "```python\n",
        "def cosine(u, v):\n",
        "    u = normalize(u)\n",
        "    v = normalize(v)\n",
        "    return u.dot(v)\n",
        "```\n",
        "For normalization, we first compute the vector norm (its length), and then divide the vector by it:\n",
        "\n",
        "```python\n",
        "def normalize(u):\n",
        "    norm = np.sqrt(u.dot(u))\n",
        "    return u / norm\n",
        "```\n",
        "(where np is import numpy as np)\n",
        "\n",
        "Or we can simplify it:\n",
        "\n",
        "```python\n",
        "def cosine(u, v):\n",
        "    u_norm = np.sqrt(u.dot(u))\n",
        "    v_norm = np.sqrt(v.dot(v))\n",
        "    return u.dot(v) / (u_norm * v_norm)\n",
        "```\n",
        "Now let's use this function to compute the A->Q->A cosine similarity.\n",
        "We will use the results from our gpt-4o-mini evaluations:\n",
        "\n",
        "```python\n",
        "results_url = url_prefix + 'rag_evaluation/data/results-gpt4o-mini.csv'\n",
        "df_results = pd.read_csv(results_url)\n",
        "```\n",
        "When creating embeddings, we will use a simple way - the same we used in the Embeddings section:\n",
        "\n",
        "```python\n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(min_df=3),\n",
        "    TruncatedSVD(n_components=128, random_state=1)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac2996b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/selamsew/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
            "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
            "/Users/selamsew/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average cosine similarity: 0.75\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def cosine(u, v):\n",
        "    u_norm = np.sqrt(u.dot(u))\n",
        "    v_norm = np.sqrt(v.dot(v))\n",
        "    return u.dot(v) / (u_norm * v_norm)\n",
        "\n",
        "# Load CSV from correct raw GitHub URL\n",
        "url_prefix = \"https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/\"\n",
        "results_url = url_prefix + 'rag_evaluation/data/results-gpt4o-mini.csv'\n",
        "df_results = pd.read_csv(results_url)\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    TfidfVectorizer(min_df=3),\n",
        "    TruncatedSVD(n_components=128, random_state=1)\n",
        ")\n",
        "\n",
        "combined_text = pd.concat([\n",
        "    df_results[\"answer_llm\"],\n",
        "    df_results[\"answer_orig\"],\n",
        "    df_results[\"question\"]\n",
        "])\n",
        "pipeline.fit(combined_text)\n",
        "\n",
        "similarities = []\n",
        "for i in range(len(df_results)):\n",
        "    \n",
        "    v_llm = pipeline.transform([df_results.loc[i, \"answer_llm\"]])[0]\n",
        "    v_orig = pipeline.transform([df_results.loc[i, \"answer_orig\"]])[0]\n",
        "    \n",
        "    sim = cosine(v_llm, v_orig)\n",
        "    \n",
        "    similarities.append(sim)\n",
        "\n",
        "average_cosine = np.mean(similarities)\n",
        "print(f\"Average cosine similarity: {average_cosine:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b073bb",
      "metadata": {},
      "source": [
        "# `Q5-Answer -> Average cosine similarity: 0.75`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c73296b",
      "metadata": {},
      "source": [
        "# Q6. Rouge\n",
        "\n",
        "And alternative way to see how two texts are similar is ROUGE.\n",
        "\n",
        "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
        "\n",
        "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
        "\n",
        "We don't need to implement it ourselves, there's a python package for it:\n",
        "\n",
        "```pip install rouge```\n",
        "\n",
        "Let's compute the ROUGE score between the answers at the index 10 of our dataframe `(doc_id=5170565b)`\n",
        "\n",
        "\n",
        "```python\n",
        "    from rouge import Rouge\n",
        "    rouge_scorer = Rouge()\n",
        "\n",
        "    r = df_results.iloc[10]\n",
        "    scores = rouge_scorer.get_scores(r.answer_llm, r.answer_orig)[0]\n",
        "    scores\n",
        "```\n",
        "\n",
        "\n",
        "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
        "\n",
        "`rouge-1` - the overlap of unigrams,\n",
        "`rouge-2` - bigrams,\n",
        "`rouge-l` - the longest common subsequence\n",
        "\n",
        "For the 10th document, Rouge-1 F1 score is 0.45\n",
        "\n",
        "Let's compute it for the pairs in the entire dataframe. What's the average Rouge-1 F1?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "97553754",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /Users/selamsew/opt/anaconda3/lib/python3.9/site-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f2936dbc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge-1': {'r': 0.45454545454545453,\n",
              "  'p': 0.45454545454545453,\n",
              "  'f': 0.45454544954545456},\n",
              " 'rouge-2': {'r': 0.21621621621621623,\n",
              "  'p': 0.21621621621621623,\n",
              "  'f': 0.21621621121621637},\n",
              " 'rouge-l': {'r': 0.3939393939393939,\n",
              "  'p': 0.3939393939393939,\n",
              "  'f': 0.393939388939394}}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "rouge_scorer = Rouge()\n",
        "\n",
        "r = df_results.iloc[10]\n",
        "scores = rouge_scorer.get_scores(r.answer_llm, r.answer_orig)[0]\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c722c65e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Rouge-1 F1 score: 0.35\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Rouge scorer\n",
        "rouge_scorer = Rouge()\n",
        "\n",
        "\n",
        "# List to store Rouge-1 F1 scores\n",
        "rouge1_f1_scores = []\n",
        "\n",
        "for i in range(len(df_results)):\n",
        "    llm_answer = df_results.loc[i, 'answer_llm']\n",
        "    orig_answer = df_results.loc[i, 'answer_orig']\n",
        "    \n",
        "    # Compute rouge scores between the two texts\n",
        "    scores = rouge_scorer.get_scores(llm_answer, orig_answer)[0]\n",
        "    \n",
        "    # Extract the Rouge-1 F1 score\n",
        "    rouge1_f1 = scores['rouge-1']['f']\n",
        "    rouge1_f1_scores.append(rouge1_f1)\n",
        "\n",
        "# Calculate average Rouge-1 F1 score\n",
        "average_rouge1_f1 = np.mean(rouge1_f1_scores)\n",
        "print(f\"Average Rouge-1 F1 score: {average_rouge1_f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "302c19da",
      "metadata": {},
      "source": [
        "# `Q5-Answer -> Average Rouge-1 F1 score: 0.35`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430bda80",
      "metadata": {},
      "source": [
        "### done!!!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
